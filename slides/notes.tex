% !TEX program = xelatex
\documentclass[9pt]{beamer}
\usepackage{hyperref}
\usepackage[T1]{fontenc}
\usepackage{fontspec}
\usefonttheme[onlymath]{serif}
% \setmainfont{[LXGWWenKai-Regular.ttf]}
% \setmainfont{Times New Roman}
% \setsansfont{LXGWWenKai-Regular.ttf}[Path=../../fonts/]
\usepackage{mathpazo}
\usepackage{libertine}
\usepackage[libertine]{newtxmath}
% \setCJKmainfont{[LXGWWenKai-Regular.ttf]}
% \setCJKsansfont{[../../fonts/LXGWWenKai-Regular.ttf]}

% \usepackage[orientation=landscape,size=custom,width=16,height=9,scale=0.4,debug]{beamerposter}
% 修改 slides 比例，

% other packages
\usepackage{latexsym,amsmath,xcolor,multicol,booktabs,calligra}
\usepackage{graphicx,pstricks,listings,stackengine,cite}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{xcolor}
\usepackage{amsthm}
\newtheorem{proposition}{Proposition}
\usepackage{subcaption}
\usepackage{multirow}

\renewcommand{\algorithmicrequire}{ \textbf{Input:}} %Use Input in the format of Algorithm
\renewcommand{\algorithmicensure}{ \textbf{Output:}} %UseOutput in the format of Algorithm
% for break
\newcommand{\algorithmicbreak}{\textbf{break}}
\newcommand{\BREAK}{\STATE \algorithmicbreak}

\setbeamertemplate{bibliography item}[text]
\bibliographystyle{plain}
% 如果参考文献太多的话，可以像下面这样调整字体：
% \tiny\bibliographystyle{plain}

\author{Yixiao Qian}
\title{Large Models are Injective and Hence Invertible}
% \subtitle{Paper Reading Seminar}
\institute{School of Mathematics, Zhejiang University}
\date{\today}
\usepackage{College}

% defs
\def\cmd#1{\texttt{\color{red}\footnotesize $\backslash$#1}}
\def\env#1{\texttt{\color{blue}\footnotesize #1}}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\definecolor{halfgray}{gray}{0.55}

% Fix for quote characters in beamer
\usepackage{upquote}

\lstset{
    basicstyle=\ttfamily\small,
    keywordstyle=\bfseries\color{deepblue},
    emphstyle=\ttfamily\color{deepred},    % Custom highlighting style
    stringstyle=\color{deepgreen},
    numbers=left,
    numberstyle=\small\color{halfgray},
    rulesepcolor=\color{red!20!green!20!blue!20},
    frame=shadowbox,
}


\begin{document}

\begin{frame}
  \titlepage
  \begin{figure}[htpb]
    \begin{center}
      \vspace*{-0.5cm}
      \includegraphics[width=0.2\linewidth]{pic/ZJU.png}
    \end{center}
  \end{figure}
\end{frame}

\begin{frame}{Table of Contents}
  \tableofcontents
\end{frame}

% ============================================================
% Section 1: Introduction & Motivation
% ============================================================
\section{Introduction \& Motivation}

\begin{frame}{Background: LLM Representations}

  \begin{itemize}
    \item LLMs use Transformer architectures and 
      map input sequences to hidden representations.
    \item Transformer architectures rely heavily on non-linearities,
      normalization, and many-to-one attentions.
    \item \textbf{Key Question}: 
      Are these representations \textbf{lossless}?
      Does different inputs collapse to same representation?
  \end{itemize}

  \begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{png/LLM-Noninjective.png}
    % \caption{Illustration of the losslessness of representations.}
  \end{figure}

\end{frame}

\begin{frame}{Research Question}
  \begin{columns}
    \begin{column}{0.5\textwidth}
      \begin{block}{Common Assumption}
        It is often assumed that attention \textbf{discards information}:
        different inputs could collapse to the same hidden state.
      \end{block}

      \vspace{0.5em}

      This paper challenges the conventional view by proving:
      \begin{enumerate}
        \item Transformer LMs are \textbf{injective} (almost surely)
        \item Hidden representations can be \textbf{inverted} to recover exact input
        \item Practical algorithm \textsc{SipIt} for prompt reconstruction
      \end{enumerate}

      \vspace{0.5em}

    \end{column}

    \begin{column}{0.48\textwidth}
      \begin{figure}
        \centering
        \includegraphics[width=\textwidth]{../injectivity/figures/teaser.pdf}
        \caption{The map from prompts to latent space is injective. \textsc{SipIt} inverts it.}
      \end{figure}
    \end{column}
  \end{columns}

\end{frame}

\begin{frame}{Approach Overview}
  \textbf{Analytic viewpoint}
  \begin{itemize}
    \item Model Transformers as \textbf{smooth functions of parameters} (embeddings, LayerNorm, attention, MLPs, residuals).
    \item Use real-analyticity to show \textbf{collision sets have measure zero}.
    \item Gradient descent never enters the exceptional set, so \textbf{injectivity persists during training}.
  \end{itemize}

  \vspace{0.5em}

  \textbf{Empirical evidence}
  \begin{itemize}
    \item 5B+ prompt pairs across GPT-2, Gemma, Llama, Mistral, Phi families: \textbf{zero collisions}.
    \item Exhaustive GPT-2 stress test (343B comparisons) keeps \textbf{pairwise distances well above $0$}.
    \item \textsc{SipIt} reconstructs prompts exactly in linear time, confirming \textbf{invertibility in practice}.
  \end{itemize}

\end{frame}

% ============================================================
% Section 2: Main Theoretical Results - Injectivity
% ============================================================
\section{Theoretical Results: Injectivity}

\begin{frame}{Last-Token Hidden State}
  \begin{columns}
    \begin{column}{0.52\textwidth}
      \begin{itemize}
        \item Prompt $\mathrm{s} = (t_1,\dots,t_\ell)$ feeds $\ell$ tokens into each of the $L$ decoder blocks; every block outputs a length-$\ell$ matrix $H^{(l)}$.
        \item The final block produces $H^{(L)} = (h^{(L)}_1,\dots,h^{(L)}_\ell)$; the last vector $h^{(L)}_\ell$ is the last-token hidden state $\mathbf{r}(\mathrm{s}; \boldsymbol{\theta})$.
        \item $\mathbf{r}$ is the input to the LM head/logits—if two prompts share it, the model cannot distinguish them.
        \item Therefore injectivity of $\mathrm{s} \mapsto \mathbf{r}(\mathrm{s}; \boldsymbol{\theta})$ is the property we analyze and test.
      \end{itemize}
    \end{column}
    \begin{column}{0.45\textwidth}
      \begin{figure}
        \centering
        \includegraphics[width=\textwidth]{png/Decoder-Only.png}
        \caption{Last-token state is the decoder output feeding the LM head.}
      \end{figure}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Key Insight: Real Analyticity}
  \begin{columns}
    \begin{column}{0.58\textwidth}
      Let $\mathrm{s} \in \mathcal{V}^{\le K}$ be a prompt, $\boldsymbol{\theta}$ the Transformer parameters, and $\mathbf{r}(\mathrm{s}; \boldsymbol{\theta})$ the last-token hidden state.
      \begin{theorem}[Transformers are real-analytic]
        For embedding dimension $d$, context length $K$, and real-analytic activation (e.g., GELU, tanh),
        the map $(\mathrm{s}, \boldsymbol{\theta}) \mapsto \mathbf{r}(\mathrm{s}; \boldsymbol{\theta}) \in \mathbb{R}^d$
        is real-analytic in parameters $\boldsymbol{\theta}$.
      \end{theorem}

      \vspace{0.5em}
      \textbf{Why it matters:}
      \begin{itemize}
        \item Real-analytic functions are either identically zero or their zero set has measure zero.
        \item Non-constant real-analytic functions have measure-zero collision sets.
        \item Standard initializations avoid measure-zero sets with probability 1.
      \end{itemize}
    \end{column}
    \begin{column}{0.4\textwidth}
      \begin{figure}
        \centering
        \includegraphics[width=\textwidth]{../injectivity/figures/real_analytic_fns.png}
        \caption{Two real-analytic functions and their difference. Zero sets form thin curves (measure zero).}
      \end{figure}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Almost-Sure Injectivity at Initialization}
  \begin{theorem}[Injectivity at initialization]
    Let $\boldsymbol{\theta}$ be drawn from any distribution with density (e.g., Gaussian).
    Denote probability over this draw by $\Pr[\cdot]$. Then for distinct prompts $\mathrm{s}, \mathrm{s}' \in \mathcal{V}^{\leq K}$:
    \[
      \Pr[\mathbf{r}(\mathrm{s}; \boldsymbol{\theta}) = \mathbf{r}(\mathrm{s}'; \boldsymbol{\theta})] = 0
    \]
  \end{theorem}

  \vspace{1em}
  \textbf{Proof sketch:}
  \begin{itemize}
    \item Define $h(\boldsymbol{\theta}) = \|\mathbf{r}(\mathrm{s}; \boldsymbol{\theta}) - \mathbf{r}(\mathrm{s}'; \boldsymbol{\theta})\|_2^2$
    \item Show $h$ is real-analytic (by composition)
    \item Construct parameter settings where $\mathbf{r}(\mathrm{s}) \neq \mathbf{r}(\mathrm{s}')$
    \item Therefore $h \not\equiv 0 \Rightarrow$ zero set has measure zero
  \end{itemize}
\end{frame}

\begin{frame}{Injectivity Preserved Under Training}
  The following theorem shows that gradient descent with standard step sizes keeps almost all parameters outside the measure-zero collision set.

  \begin{theorem}[Training preserves injectivity]
    After $T$ steps of gradient descent with step sizes in $(0,1)$, with probability one:
    \[
      \mathrm{s} \neq \mathrm{s}' \Longrightarrow \mathbf{r}(\mathrm{s}; \boldsymbol{\theta}_T) \neq \mathbf{r}(\mathrm{s}'; \boldsymbol{\theta}_T)
    \]
  \end{theorem}

  \vspace{1em}
  \textbf{Key ideas:}
  \begin{itemize}
    \item GD update $\phi(\boldsymbol{\theta}) = \boldsymbol{\theta} - \eta \nabla \mathcal{L}(\boldsymbol{\theta})$ is real-analytic because the representation map $\mathbf{r}$ and the loss $\mathcal{L}$ are real-analytic in $\boldsymbol{\theta}$.
    \item A set $\mathcal{C}$ with measure zero, its pre-image $\mathcal{C}_1 = \phi^{-1}(\mathcal{C})$ must also have measure zero. 
    \begin{itemize}
      \item Jacobian determinant $\det D\phi$ not identically zero
      \item Inverse Function Theorem: preserves absolute continuity
    \end{itemize}
    \item Finite composition of GD steps preserves measure properties.
  \end{itemize}
\end{frame}

% ============================================================
% Section 3: Invertibility - The SipIt Algorithm
% ============================================================
\section{Invertibility: The \textsc{SipIt} Algorithm}

\begin{frame}{From Injectivity to Invertibility}
  
  Invertibility usually requires bijectivity, but we only need to invert the map on the hidden states that \emph{actually occur}:

  \begin{center}
    \textbf{Injectivity $\Rightarrow$ Hidden states uniquely identify inputs}
  \end{center}

  \vspace{1em}
  But can we \emph{actually recover} the input from hidden states?

  \vspace{1em}
  \begin{block}{\textsc{SipIt}: Sequential Inverse Prompt via Iterative Updates}
    Algorithm that reconstructs exact input text from hidden activations
    \begin{itemize}
      \item Exploits causal structure of Transformers
      \item Guaranteed linear-time complexity: $O(T \cdot |\mathcal{V}|)$
      \item Often much faster in practice
    \end{itemize}
    where $T$ is the length of the input sequence and $|\mathcal{V}|$ is the size of the vocabulary.
  \end{block}
\end{frame}

\begin{frame}{Key Idea: Causal Structure}
  \textbf{Observation:} Causal attention makes each hidden state depend only on the prefix and current token:
  \[
  \mathbf{h}_t = F(\mathrm{s}_t \,;\, \mathrm{s}_1,\dots,\mathrm{s}_{t-1})\,.
  \]

  \vspace{0.6em}
  \textbf{Sequential recovery:}
  The hidden states are $\mathbf{h}_1, \mathbf{h}_2, \cdots, \mathbf{h}_T$, then
  \[
  \begin{aligned}
    t=1: &\quad \mathbf{h}_1 \Rightarrow \mathrm{s}_1 \\
    t=2: &\quad (\mathrm{s}_1,\, \mathbf{h}_2) \Rightarrow \mathrm{s}_2 \\
    &\quad \vdots \\
    t=T: &\quad (\mathrm{s}_{<T},\, \mathbf{h}_T) \Rightarrow \mathrm{s}_T
  \end{aligned}
  \]

  \vspace{0.6em}
  \textbf{Strategy:}
  \begin{enumerate}
    \item Reconstruct tokens left-to-right, maintaining the recovered prefix.
    \item At step $t$, enumerate candidates consistent with the prefix.
    \item Compare $F(v;\text{prefix})$ to $\widehat{\mathbf{h}}_t$; injectivity guarantees exactly one match.
  \end{enumerate}
\end{frame}

\begin{frame}{Gradient-Guided Policy (Why \textsc{SipIt} is Fast)}

  \textbf{Gradient-Guided Policy:} Let $F(\mathbf{e}; \pi, t)$ be the hidden state produced by running the decoder (up to layer $\ell$) on prefix $\pi$ with a continuous proxy embedding $\mathbf{e}$ at position $t$.
  \[
    \mathcal{L}(\mathbf{e}) = \tfrac12 \|F(\mathbf{e}; \pi, t) - \widehat{\mathbf{h}}_t\|_2^2
  \]
  \[
    \mathbf{e}^{(j)} \leftarrow \mathbf{e}^{(j-1)} - \gamma \nabla_{\mathbf{e}} \mathcal{L}
  \]
  where $\pi=\langle \mathrm{s}_1,\dots,\mathrm{s}_{t-1} \rangle$ is the prompt,
  and $\widehat{\mathbf{h}}_t$ is the observed hidden state at position $t$.

  \vspace{0.4em}
  \textbf{Nearest-neighbor verification}
  \begin{itemize}
    \item Rank tokens by $\|\mathbf{E}_v - \mathbf{e}^{(j)}\|_2$ and test nearest first.
    \item Usually locks onto the correct token after a handful of proposals (vs full vocabulary scan).
  \end{itemize}

  \vspace{0.4em}
  \textbf{Result:} $10\text{--}100\times$ fewer trials while keeping the same linear-time worst-case guarantee.
\end{frame}

\begin{frame}[fragile]{The \textsc{SipIt} Algorithm}
  \begin{algorithm}[H]
    \caption{\textsc{SipIt}}
    \begin{algorithmic}[1]
      \REQUIRE Observed hidden states $\widehat{\mathbf{H}}^{(\ell)}$, vocabulary $\mathcal{V}$, tolerance $\varepsilon$
      \ENSURE Recovered sequence $\widehat{\mathrm{s}}$
      \STATE $\widehat{\mathrm{s}} \gets \langle \rangle$
      \FOR{$t = 1$ to $T$}
        \FOR{$j = 1$ to $|\mathcal{V}|$}
          \STATE $v_j \gets \textsc{Policy}(\mathcal{V}, \mathcal{C}, \widehat{\mathrm{s}}, \ell)$
          \IF{$\widehat{\mathbf{h}}_t$ matches candidate $v_j$ (within $\varepsilon$)}
            \STATE $\widehat{\mathrm{s}} \gets \widehat{\mathrm{s}} \oplus v_j$
            \STATE \textbf{break}
          \ENDIF
        \ENDFOR
      \ENDFOR
      \RETURN $\widehat{\mathrm{s}}$
    \end{algorithmic}
  \end{algorithm}
  Here $\ell$ is the layer index, $T$ is the sequence length, and $|\mathcal{V}|$ is the vocabulary size.

\end{frame}

% ============================================================
% Section 4: Experimental Validation
% ============================================================
\section{Experimental Validation}

\begin{frame}{Collision Search Experiments}
  \textbf{Setup:}
  \begin{itemize}
    \item 100k prompts from Wikipedia, C4, The Pile, Python code
    \item ~5 billion pairwise comparisons
    \item Models: GPT-2 family, Gemma-3 family, Llama-3.1, Mistral, Phi-4
  \end{itemize}

  \vspace{0.5em}
  \textbf{Result:} \textcolor{red}{\textbf{Zero collisions}} found across all models and layers

  \begin{figure}
    \centering
    \includegraphics[width=0.85\textwidth]{../injectivity/figures/combined_swarm_boxplot.pdf}
    \caption{Minimum distances between last-token states across layers. All distances $\gg 10^{-6}$.}
  \end{figure}
\end{frame}

\begin{frame}{Exhaustive Collision Test}
  \begin{columns}
    \begin{column}{0.48\textwidth}
      \textbf{Stress test:}
      \begin{itemize}
        \item Select 10 closest prompt pairs from dataset
        \item Append \emph{every} vocabulary token to each
        \item Compute all pairwise distances
        \item Result: \textbf{343+ billion} prompt pairs tested
      \end{itemize}

      \vspace{0.5em}
      \textbf{Findings:}
      \begin{itemize}
        \item Still no collisions found
        \item All distances well above zero
        \item Confirms local injectivity
      \end{itemize}
    \end{column}
    \begin{column}{0.5\textwidth}
      \begin{figure}
        \centering
        \includegraphics[width=\textwidth]{../injectivity/figures/exhaustive-gpt2-boxplot.pdf}
        \caption{Exhaustive test on GPT-2: all minima stay well above collision threshold.}
      \end{figure}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Distance vs Sequence Length}
  \begin{columns}
    \begin{column}{0.48\textwidth}
      \begin{figure}
        \centering
        \includegraphics[width=\textwidth]{../injectivity/figures/gpt2-distances-vs-length.pdf}
        \caption{GPT-2: distances grow at short lengths then stabilize.}
      \end{figure}
    \end{column}
    \begin{column}{0.48\textwidth}
      \begin{figure}
        \centering
        \includegraphics[width=\textwidth]{../injectivity/figures/gemma-3-1b-pt-distances-vs-length.pdf}
        \caption{Gemma-3: similar pattern across sequence lengths.}
      \end{figure}
    \end{column}
  \end{columns}

  \vspace{0.5em}
  \small{\textbf{Observation:} Minimum distance never close to zero at any length; grows rapidly at short lengths then levels off.}
\end{frame}

\begin{frame}{Invertibility: \textsc{SipIt} Performance}
  \begin{columns}
    \begin{column}{0.55\textwidth}
      \textbf{Setup:}
      \begin{itemize}
        \item 100 prompts (90\% natural, 10\% random)
        \item Reconstruct from hidden states
        \item Compare methods
      \end{itemize}

      \vspace{0.5em}
      \textbf{Results on GPT-2 Small:}
      \begin{center}
      \scriptsize
      \begin{tabular}{lcc}
        \toprule
        Method & Time (s) & Acc \\
        \midrule
        \textsc{HardPrompts} & 6132.59 & 0.00 \\
        Brute Force & 3889.61 & 1.00 \\
        \textsc{SipIt} & \textbf{28.01} & \textbf{1.00} \\
        \bottomrule
      \end{tabular}
      \end{center}

      \vspace{0.3em}
      \small{\textsc{SipIt}: \textbf{100\%} accuracy, \textbf{100$\times$} faster}
    \end{column}
    \begin{column}{0.43\textwidth}
      \begin{figure}
        \centering
        \includegraphics[width=\textwidth]{../injectivity/figures/sipit_layer_ablation.pdf}
        \caption{Inversion time vs layer depth: graceful scaling.}
      \end{figure}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Additional Results: Multiple Models}
  \begin{center}
  \small
  \begin{tabular}{lccc}
    \toprule
    \multirow{2}{*}{Model} & \multicolumn{3}{c}{L2 Distance (minimum)} \\
    \cmidrule(r){2-4}
    & Layer 1 & Layer $L/2$ & Layer $L$ \\
    \midrule
    Llama-3.1-8B & 0.001 & 0.129 & 0.620 \\
    Mistral-7B-v0.1 & 0.002 & 0.187 & 1.274 \\
    Phi-4-mini-instruct & 0.014 & 1.336 & 9.020 \\
    TinyStories-33M & 0.029 & 1.434 & 2.793 \\
    \bottomrule
  \end{tabular}
  \end{center}

  \vspace{1em}
  \textbf{Key observations:}
  \begin{itemize}
    \item All minimum distances $\gg 10^{-6}$ (collision threshold)
    \item Distances typically increase with depth
    \item Consistent across different model sizes and architectures
  \end{itemize}
\end{frame}

% ============================================================
% Section 5: Implications and Discussion
% ============================================================
\section{Implications \& Discussion}

\begin{frame}{Key Takeaways}
  \begin{enumerate}
    \item \textbf{Theoretical:} Transformer LMs are structurally injective
    \begin{itemize}
      \item Almost-sure property from real analyticity
      \item Preserved during training
      \item Not an asymptotic result—holds at finite width/depth
    \end{itemize}

    \vspace{0.5em}
    \item \textbf{Practical:} Hidden states are invertible
    \begin{itemize}
      \item \textsc{SipIt} achieves exact recovery in linear time
      \item Confirmed across multiple state-of-the-art models
      \item No collisions in billions of tests
    \end{itemize}

    \vspace{0.5em}
    \item \textbf{Implications:}
    \begin{itemize}
      \item Transparency: activations fully encode inputs
      \item Interpretability: can trace back to exact prompts
      \item Security: hidden states leak complete information
    \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[plain]
  \centering
  \vspace{2cm}
  \Huge Thank you!

  \vspace{2em}
  \normalsize
  Questions?
\end{frame}


\end{document}
