\section{Discussion and conclusions}

This work establishes that decoder-only Transformers are almost surely injective: distinct prompts produce distinct hidden states under standard initialization and training. Building on this structural result, we introduced \textsc{SipIt}, the first algorithm that can recover the \emph{exact} input sequence from hidden activations, with provable linear-time guarantees. Together, these contributions move injectivity from an informal belief to a rigorously grounded and operational property of language models.

%The scientific impact is clear. A common assumption has been that Transformers are inherently lossy, due to nonlinearities, normalization, and attention's many-to-one nature. Our results prove the opposite: information is preserved almost surely, not discarded. This closes a conceptual gap and provides a reusable analytic framework for reasoning about Transformer components. The constructive inversion offered by \textsc{SipIt} strengthens this point in practice, establishing a clean baseline for interpretability and auditing: if probes or inversion methods fail, it is not because the information is missing. For mechanistic interpretability in particular, injectivity guarantees that last-token states faithfully encode the full input, giving a sound foundation for causal and probing analyses.

The scientific impact is clear. Our findings reconcile two competing views in the community: Transformers as “lossy” due to nonlinearities, normalization, and many-to-one attention, versus language models as injective in their hidden representations. We advocate viewing language models as maps on the \emph{sequence} space rather than the embedding space; under this perspective, we prove that all information about the input sequence is almost surely preserved end-to-end.  The constructive inversion offered by \textsc{SipIt} strengthens this point in practice, establishing a clean baseline for interpretability and auditing: if probes or inversion methods fail, it is not because the information is missing. For mechanistic interpretability in particular, injectivity guarantees that last-token states faithfully encode the full input, giving a sound foundation for causal and probing analyses.

Beyond theory, the findings carry practical and legal implications. Hidden states are not abstractions but the prompt in disguise. Any system that stores or transmits them is effectively handling user text itself. This affects privacy, deletion, and compliance: even after prompt deletion, embeddings retain the content. Regulators have sometimes argued otherwise; for example, the Hamburg Data Protection Commissioner claimed that weights do not qualify as personal data since training examples cannot be trivially reconstructed \citep{hamburg}. Our results show that at inference time user inputs remain fully recoverable. There is no ``free privacy'' once data enters a Transformer.

Finally, this work opens several directions. Extending the analysis to multimodal architectures such as music and vision Transformers is an open problem. Studying approximate inversion under noise or quantization will clarify how robust invertibility remains in practice. Bridging these technical insights with evolving regulatory frameworks will be crucial for safe and responsible deployment.

%We proved that decoder-only Transformer language models are almost surely injective: distinct prompts produce distinct hidden representations both at initialization and throughout training. Building on this property, we introduced \textsc{SipIt}, the first algorithm to guarantee exact recovery of an input sequence from hidden states in provable linear time. This elevates injectivity from a theoretical property to an operational tool, with direct implications for interpretability, transparency, and safe deployment.

%Our findings also raise important legal and policy considerations. Some regulators, such as the Hamburg Data Protection Commissioner \citep{hamburg}, have argued that model weights do not constitute personal data since inputs cannot be trivially recovered. By contrast, our results show that \emph{runtime representations are fully reversible}. This means hidden states should be treated as equivalent to the original prompt: sharing or storing them without safeguards could expose user data, and deletion mandates must apply to embeddings as well as raw text. Failing to do so may create legal liability.

%Looking ahead, we see several directions for future work. First, quantifying the robustness of \textsc{SipIt} under noise, compression, or quantization would extend its practical relevance. Second, exploring how invertibility interacts with alignment and interpretability tools could sharpen our understanding of how LLMs encode meaning. Third, extending the analysis beyond language to other modalities (audio, images, etc.) remains an important open question. Finally, engaging with legal scholars and policymakers is crucial to ensure that regulations account for the reversibility of hidden states in modern LLMs.